{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40db8926",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d74eecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: image not found: dataset/images\\classes.jpg\n",
      "✅ Done! Saved 167 training and 42 validation crops.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# === Config ===\n",
    "image_dir = 'dataset/images'\n",
    "label_dir = 'dataset/labels'\n",
    "output_root = 'dataset_vit'\n",
    "train_split = 0.8\n",
    "resize_dim = (224, 224)  # Vision Transformer input size\n",
    "\n",
    "# === Setup ===\n",
    "train_dir = os.path.join(output_root, 'train')\n",
    "val_dir = os.path.join(output_root, 'val')\n",
    "for split_dir in [train_dir, val_dir]:\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "# === Gather Crops ===\n",
    "crops = []  # Will store (image_crop, class_id) tuples\n",
    "\n",
    "for label_file in os.listdir(label_dir):\n",
    "    if not label_file.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    image_file = label_file.replace(\".txt\", \".jpg\")\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: image not found: {image_path}\")\n",
    "        continue\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # Parse YOLO label\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            class_id, cx, cy, bw, bh = map(float, parts)\n",
    "\n",
    "            # Convert normalized coordinates to pixel values\n",
    "            x1 = int((cx - bw/2) * w)\n",
    "            y1 = int((cy - bh/2) * h)\n",
    "            x2 = int((cx + bw/2) * w)\n",
    "            y2 = int((cy + bh/2) * h)\n",
    "\n",
    "            # Clamp and crop\n",
    "            x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Resize for ViT\n",
    "            crop_resized = cv2.resize(crop, resize_dim)\n",
    "\n",
    "            # Store for later split\n",
    "            crops.append((crop_resized, str(int(class_id))))\n",
    "\n",
    "# === Shuffle and Split ===\n",
    "random.shuffle(crops)\n",
    "split_idx = int(len(crops) * train_split)\n",
    "train_crops = crops[:split_idx]\n",
    "val_crops = crops[split_idx:]\n",
    "\n",
    "# === Save Crops ===\n",
    "def save_crops(crop_list, base_dir):\n",
    "    counter = {}\n",
    "    for img, class_id in crop_list:\n",
    "        class_dir = os.path.join(base_dir, class_id)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "        counter[class_id] = counter.get(class_id, 0) + 1\n",
    "        filename = f\"{class_id}_{counter[class_id]}.jpg\"\n",
    "        cv2.imwrite(os.path.join(class_dir, filename), img)\n",
    "\n",
    "save_crops(train_crops, train_dir)\n",
    "save_crops(val_crops, val_dir)\n",
    "\n",
    "print(f\"✅ Done! Saved {len(train_crops)} training and {len(val_crops)} validation crops.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfe92f",
   "metadata": {},
   "source": [
    "# Transformer Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db89177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transforms (ViT expects 224x224 and normalized input)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)  # or use ImageNet mean/std if using ImageNet-pretrained ViT\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('dataset_vit/train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('dataset_vit/val', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b598a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hanju\\tf-gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hanju\\tf-gpu\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hanju\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg2_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pretrained ViT base model and modify the classifier head\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, len(train_dataset.classes))  # Adjust output classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7285bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2b30e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:16<00:00, 12.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:09<00:00, 11.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.4282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:09<00:00, 11.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:07<00:00, 11.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.3004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:05<00:00, 10.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:09<00:00, 11.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.1324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:09<00:00, 11.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:06<00:00, 11.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:09<00:00, 11.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:07<00:00, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a6ac2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18272b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'vit_currency_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d747936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())  # needed for mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81ffbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       1.00      1.00      1.00        17\n",
      "          20       1.00      1.00      1.00        12\n",
      "          50       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=val_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e171f03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (mAP): 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_true_bin = label_binarize(all_labels, classes=list(range(len(val_dataset.classes))))\n",
    "y_score = np.array(all_probs)\n",
    "\n",
    "mAP = average_precision_score(y_true_bin, y_score, average='macro')\n",
    "print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234f1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "video_path = r'testing_video\\NZD10_resized_video_640x640.mp4'\n",
    "output_dir = r'cropped_frames'\n",
    "crop_size = (224, 224)  # Resize for ViT input\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Open Video ===\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_idx = 0\n",
    "crop_idx = 0\n",
    "\n",
    "roi = None\n",
    "drawing = False\n",
    "ix, iy = -1, -1\n",
    "\n",
    "def draw_rectangle(event, x, y, flags, param):\n",
    "    global ix, iy, roi, drawing\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ix, iy = x, y\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE and drawing:\n",
    "        roi = (ix, iy, x, y)\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        roi = (ix, iy, x, y)\n",
    "\n",
    "cv2.namedWindow(\"Frame\")\n",
    "cv2.setMouseCallback(\"Frame\", draw_rectangle)\n",
    "\n",
    "# === Main Loop ===\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    temp_frame = frame.copy()\n",
    "    if roi:\n",
    "        x1, y1, x2, y2 = roi\n",
    "        cv2.rectangle(temp_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", temp_frame)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == ord('c') and roi:\n",
    "        # Crop and resize ROI\n",
    "        x1, y1, x2, y2 = roi\n",
    "        x1, x2 = sorted([x1, x2])\n",
    "        y1, y2 = sorted([y1, y2])\n",
    "        cropped = frame[y1:y2, x1:x2]\n",
    "        if cropped.size > 0:\n",
    "            resized = cv2.resize(cropped, crop_size)\n",
    "            filename = f\"crop_{frame_idx:04d}_{crop_idx:02d}.jpg\"\n",
    "            cv2.imwrite(os.path.join(output_dir, filename), resized)\n",
    "            print(f\"✅ Saved: {filename}\")\n",
    "            crop_idx += 1\n",
    "        roi = None\n",
    "\n",
    "    elif key == ord('n'):\n",
    "        # Next frame\n",
    "        frame_idx += 1\n",
    "        crop_idx = 0\n",
    "        roi = None\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
